{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD - SOLUTION\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rdd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task. You can use the `sample()` method to extract just a sample of the data or use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). \n",
    "\n",
    "The `textFile` function returns data in strings. This should work fine for this lab.\n",
    "\n",
    "Other methods you use might return data in type `Byte`. If you haven't used Python `Byte` types before, google it. You can convert a value of `x` type byte into e.g. a UTF8 string using `x.decode('uft-8')`. Alternatively, you can use the `open` method of the gzip library to read in all the lines as UTF-8 strings like this:\n",
    "```\n",
    "import gzip\n",
    "with gzip.open('cite75_99.txt.gz', 'rt',encoding='utf-8') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines() )\n",
    "```\n",
    "This is less efficient than using `textFile` because `textFile` would use the underlying HDFS or other file system to read the file across all the worker nodes while the using `gzip.open()...readlines()` will read all the data in the frontend and then distribute it to all the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add .sample(False, 0.05).cache() while debugging, then run on full dataset\n",
    "\n",
    "rddCitations = sc.textFile(\"cite75_99.txt.gz\")#.sample(False, 0.05).cache()\n",
    "rddPatents = sc.textFile(\"apat63_99.txt.gz\")#.sample(False, 0.05).cache()\n",
    "\n",
    "header = rddCitations.first() # extract header\n",
    "rddCitations = rddCitations.filter( lambda x: x != header) #filter out header\n",
    "\n",
    "header_pat = rddPatents.first() # extract header\n",
    "rddPatents = rddPatents.filter( lambda x: x != header_pat) # filter out header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like the following after filtering out the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3858241,956203',\n",
       " '3858241,1324234',\n",
       " '3858241,3398406',\n",
       " '3858241,3557384',\n",
       " '3858241,3634889']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCitations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,',\n",
       " '3070805,1963,1096,,\"US\",\"CA\",,1,,2,6,63,,1,,0,,,,,,,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, they are a single string with multiple CSV's. You will need to convert these to (K,V) pairs, probably convert the keys to `int` and so on. You'll need to `filter` out the header string as well since there's no easy way to extract all the lines except the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Solution Below: We need to join citations with patents on citation #, then get each cited patents state, so later we can augment and find what we want**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up our RDDs a bit\n",
    "\n",
    "citations = rddCitations.map( lambda x: x.split(',') )\\\n",
    "    .map( lambda x: (int(x[0]), int(x[1])) )\n",
    "    #.filter( lambda x: x[0] != '\"\"' )\n",
    "    #.groupByKey()\n",
    "    #.take(20)\n",
    "\n",
    "# Filtered out the '\"\"' entries from Patent data State column\n",
    "\n",
    "pats = rddPatents.map( lambda x: x.split(',') )\\\n",
    "    .map( lambda x: (int(x[0]), x[5]) )\\\n",
    "    .filter( lambda x: x[1] != '\"\"' ) \n",
    "    #.groupByKey()\n",
    "    #.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pats.filter( lambda x: x[0] == '3187320').take(20) # This was a tool to make sure we correctly had the Cited state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a new RDD called joined that swaps the positions of CITING and CITED and joins with Patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3606034, (3858746, '\"NY\"')),\n",
       " (3606034, (4573851, '\"NY\"')),\n",
       " (3606034, (4695103, '\"NY\"')),\n",
       " (3606034, (4789075, '\"NY\"')),\n",
       " (3515792, (3859884, '\"CA\"'))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining our citations with patents on CITED as key after reorganizing data(swapping CITING with CITED to make CITED the key)\n",
    "\n",
    "new_citations = citations.map( lambda x: (x[1], x[0]) )\n",
    "joined = new_citations.join(pats)\n",
    "\n",
    "joined.take(5)\n",
    "\n",
    "# RDD named joined gives us (Cited, (Citing, Cited State))\n",
    "# Result we want: (Cited, Cited State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE ALL OF THIS BRAINSTORMING\"\n",
    "# # Joining our citations with patents on CITING as key(no swap)\n",
    "\n",
    "# joined_alt = citations.join(pats)\n",
    "\n",
    "# joined_alt.take(5)\n",
    "\n",
    "# # Another RDD, but this time gives us (Citing, (Cited, Citing State))\n",
    "# # Result (Citing, Citing State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3858570, (3464388, '\"MO\"')),\n",
       " (3897751, (3464388, '\"MO\"')),\n",
       " (3920000, (3464388, '\"MO\"')),\n",
       " (3924571, (3464388, '\"MO\"')),\n",
       " (4034740, (3464388, '\"MO\"'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to join into one intermediate result that has everything we need since we're missing citing state\n",
    "\n",
    "# Swap again to make (Citing, Cited, Cited State)\n",
    "citing = joined.map( lambda x: (x[1][0], (x[0], x[1][1])))\n",
    "citing.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that CITING is the key, we can left outer join with the patent data to get CITING state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3938145, ((3114148, '\"NY\"'), '\"NM\"')),\n",
       " (3938145, ((3149332, '\"IA\"'), '\"NM\"')),\n",
       " (3938145, ((3175214, '\"CA\"'), '\"NM\"')),\n",
       " (3938145, ((3603919, '\"KS\"'), '\"NM\"')),\n",
       " (4975659, ((3178648, '\"IL\"'), '\"MA\"'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_table = citing.leftOuterJoin(pats)\n",
    "test_table.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have (CITING, (CITED, CITED STATE), CITING STATE) we are able to reorganize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4580865, '\"NJ\"', 3585564, '\"NY\"'),\n",
       " (4580865, '\"NJ\"', 3920304, '\"NJ\"'),\n",
       " (4580865, '\"NJ\"', 3649956, '\"CA\"'),\n",
       " (4580865, '\"NJ\"', 3112975, '\"NY\"'),\n",
       " (4580865, '\"NJ\"', 4239325, '\"OH\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_states = test_table.map( lambda x: (x[0], x[1][1], x[1][0][0], x[1][0][1]))\n",
    "citation_states.take(5)\n",
    "\n",
    "# The following is of form (CITING, CITING STATE, CITED, CITED STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that we are going to apply over the citation_states data that counts same state occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_count(data):\n",
    "    count = 0\n",
    "    \n",
    "    if data[1] == data[3]:\n",
    "        count = count + 1\n",
    "    \n",
    "    return (data[0], count) # returns tuple with CITING # and the state count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5066526, 0), (5066526, 0), (5066526, 0), (5066526, 0), (5066526, 0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_states_ct = citation_states.map(state_count)\n",
    "same_states_ct.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to \"reduce\" by using .groupByKey() to aggregate all of the counts by CITING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5386821, <pyspark.resultiterable.ResultIterable at 0x7f81bcbbcac0>),\n",
       " (4715820, <pyspark.resultiterable.ResultIterable at 0x7f81bcc65700>),\n",
       " (4329825, <pyspark.resultiterable.ResultIterable at 0x7f81d4f0cdf0>),\n",
       " (5107227, <pyspark.resultiterable.ResultIterable at 0x7f81d4f0c3a0>),\n",
       " (4421259, <pyspark.resultiterable.ResultIterable at 0x7f81bcc734f0>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = same_states_ct.groupByKey()\n",
    "totals.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also remember to use mapValues(sum) to be able to see the actual values from the iterable, and sum all values for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(125, 5959466),\n",
       " (103, 5983822),\n",
       " (100, 6008204),\n",
       " (98, 5952345),\n",
       " (96, 5958954),\n",
       " (96, 5998655),\n",
       " (94, 5936426),\n",
       " (90, 5913855),\n",
       " (90, 5925042),\n",
       " (90, 5951547)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reduction = totals.mapValues(sum).map( lambda x: (x[1], x[0])).sortByKey(False)\n",
    "final_reduction.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we have to join this final result with patents by citations as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issues appending same state counts to our patent \"table\" of data\n",
    "\n",
    "# final_rdd = final_reduction.map( lambda x: (x[1], x[0]) )\n",
    "\n",
    "# # Parse the data again to get the full rows in 2 pieces\n",
    "# new_pats = rddPatents.map( lambda x: x.split(',') )\\\n",
    "#     .map( lambda x: (int(x[0]), x[1:22]) )\n",
    "\n",
    "# final_pats = new_pats.leftOuterJoin(final_rdd)\n",
    "\n",
    "# # Lastly, reorganize\n",
    "# final_clean = final_pats.map( lambda x: (x[0], x[1][1], x[1][0])).sortBy( lambda x: x[1][0]) # x[1][0] here is the same state count, so we want it at the end\n",
    "# final_clean.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-working session notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = rddPatents.map( lambda x: x.split(',') )\\\n",
    "    .map( lambda x: (x[5], x[0]) )\\\n",
    "    .filter( lambda x: x[0] != '\"\"' )\\\n",
    "    .groupByKey()\n",
    "    #.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Notes:**\n",
    "\n",
    "Hadoop vs. PySpark - Hadoop relies too much on our processing of input and directing of output, PySpark figures out the best way to structure/optimize the flow\n",
    " of the data parallels\n",
    " \n",
    "Hadoop - Everything is a (k,v) pair\n",
    "\n",
    "PySpark - Everything is an element of an RDD\n",
    " \n",
    "Also PySpark remembers the sequence of computations we need to do in order to get to the final state we wanted. That's the benefit of the RDD, is that if a node with 3 splits of the entire data fails, it still knows the steps and can apply them to the splits of data living on other nodes\n",
    " \n",
    "Map - .map() which applies the lambda to each value x\n",
    "\n",
    "Reduce - .groupByKey() which will aggregate all of the data by key\n",
    "\n",
    "In the example above, we filter out empty state values\n",
    "\n",
    "Create RDD - .parallelize(data_structure)\n",
    "\n",
    "ie: counts.take() will need to read all of our data again unless we use .cache(), so remember to add it specifically after doing joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
